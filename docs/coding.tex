\documentclass{article}

\newcommand{\splitval}{\ensuremath{\theta}}
\newcommand{\splitvals}{\ensuremath{\Theta}}
\newcommand{\depth}{\ensuremath{D}}
\newcommand{\policytree}{\texttt{policytree}}

\title{Algorithms and implementation methods for finding optimal policy trees}
\author{James Cussens}

\begin{document}

\maketitle

\section{Introduction}
\label{sec:intro}

In this note we consider the problem of finding an optimal policy tree
for a set of \emph{units}, where a unit is a collection of covariate
values together with a reward for each possible action. First, let us
define some notation.  Let $N$ be the set $\{1,\dots,n\}$ of
units. Let $A$ be the set of available actions.  Let ${\cal F}_d$
represent the set of functions $f:N \rightarrow A$ representable by a
policy tree of depth $d$. $f(i)$ assigns an action to unit $i$, so it
is an \emph{action assignment functions}. Let $S$ be the set of splits
available, each split being defined by a choice of covariate $j$ and
a value $v_j$ for that covariate. If a unit $i$ is such that
$x_{ij} \leq v_{j}$ then we say the unit is sent left by the split,
otherwise it is sent right.

Let $s_{L}(N)$ (resp.\ $s_{R}(N)$) be the units sent left (resp.\
right) when unit-set $N$ is split using split $s \in S$. Let $r(i,a)$
be the reward for unit $i$ when it is assigned action $a$. For any
action assignment function $f$ define
$R(f,N) := \sum_{i \in N} r(i,f(i))$, so $R(f,N)$ is the reward when
using $f$ to assign actions to all individuals in $N$. Define
$f^{*}_{d,N} := \arg\max_{f \in {\cal F}_d} R(f,N)$, so $f^{*}_{d,N}$
is the best function in ${\cal F}_d$ (the best depth $d$ policy tree)
to use to assign actions to all individuals in $N$. Given $d$ and $N$
our goal is to find $f^{*}_{d,N}$.

Abbreviate $R(f^{*}_{d,N},N)$ to $R^{*}_{d,N}$, so $R^{*}_{d,N}$ is the best
possible reward---the reward resulting from using the best tree
$f^{*}_{d,N}$---for a given $d$ and $N$. We have the basic recursion
for $d>0$
\begin{equation}
  \label{eq:basicrecursion}
  R^{*}_{d,N}
  = \max_{s \in S}   R^{*}_{d-1,s_{L}(N)} + R^{*}_{d-1,s_{R}(N)} 
\end{equation}
The base case is:
\begin{equation}
  \label{eq:base}
    R^{*}_{0,N} =  \max_{a \in A} \sum_{i \in N}  r(i,a)
\end{equation}
(\ref{eq:basicrecursion}) and (\ref{eq:base}) lead to a simple
algorithm for finding $f^{*}_{d,N}$: compute $R^{*}_{d-1,s_{L}(N)} +
R^{*}_{d-1,s_{R}(N)}$ by recursion for each split $s \in S$ and record
which has the highest reward. By recording the maximising
split during the recursive computation we can recover the policy tree
which produces this maximal reward. In the rest of this note we
present optimisations to this basic algorithm and also discuss how
best to implement the algorithm.

\section{Some useful bounds}
\label{sec:bounds}

If $N = N_{1} \dot\cup N_{2}$ (where $\dot\cup$ denotes disjoint
union) then
\begin{eqnarray}
  \label{eq:genmain}
 R^{*}_{d,N_{1}} + R(f^{*}_{d,N_{1}},N_{2})  & \leq R^{*}_{d,N}  & \leq  R^{*}_{d,N_{1}} + R^{*}_{d,N_{2}} \\
 R^{*}_{d,N_{2}} + R(f^{*}_{d,N_{2}},N_{1})  & \leq R^{*}_{d,N}  & \leq  R^{*}_{d,N_{1}} + R^{*}_{d,N_{2}} 
\end{eqnarray}
The RHS inequality of (\ref{eq:genmain}) follows from the simple
observation that optimising for $N_1$ and $N_2$ separately will always
allow a summed reward which is no worse that finding a single optimal
tree for $N = N_{1} \dot\cup N_{2}$. The LHS inequalities simply state
that the reward of the best tree for $N$ cannot be worse than applying
the best tree for $N_{1}$ or $N_2$ to all of $N$.

We also have the (typically loose) bounds which  state that
the best we can hope for is for all units to be assigned their best
action and that we can always do at least as well as an optimal depth 0 tree:
\begin{equation}
  \label{eq:loosebound}
\max_{a \in A} \sum_{i \in N}  r(i,a) \leq  R^{*}_{d,N} \leq \sum_{i \in N} \max_{a \in A} r(i,a) 
\end{equation}
Also we have that if we use the best tree for $N_{1}$ to assign
actions to units in $N_2$ the worst we can do is for every unit in
$N_2$ to be assigned its worst action:
\begin{equation}
  \label{eq:llb}
\sum_{i \in N_{2}} \min_{a \in A} r(i,a) \leq R(f^{*}_{d,N_{1}},N_{2})  
\end{equation}
From (\ref{eq:genmain}) and (\ref{eq:loosebound})  we have:
\begin{equation}
  \label{eq:ub}
R^{*}_{d,N}   \leq  R^{*}_{d,N_{1}} + \sum_{i \in N_{2}} \max_{a \in A} r(i,a) 
\end{equation}
and from (\ref{eq:genmain}) and (\ref{eq:llb}) we have:
\begin{eqnarray}
  \label{eq:lb}
  R^{*}_{d,N_{1}}
  & \leq
  & R^{*}_{d,N} -  R(f^{*}_{d,N_{1}},N_{2})
  \nonumber \\
  & =
  & \sum_{i\in N_{1}} r(i, f^{*}_{d,N}(i)) + \sum_{i\in N_{2}} r(i,
    f^{*}_{d,N}(i))
    - \sum_{i\in N_{2}} r(i,
    f^{*}_{d,N_{1}}(i)) \nonumber \\
  & =
  & R(f^{*}_{d,N},N_{1})
    + \sum_{i\in N_{2}} r(i,
    f^{*}_{d,N}(i))
    - \sum_{i\in N_{2}} r(i,
    f^{*}_{d,N_{1}}(i)) \nonumber \\
  & \leq
  & R(f^{*}_{d,N},N_{1}) + \sum_{i\in N_{2}} \left(
    r(i,f^{*}_{d,N}(i)) - \min_{a \in A} r(i,a) \right)
\end{eqnarray}

\section{Optimising the algorithm}
\label{sec:optimising}

There are three simple methods for optimising the basic algorithm
given in Section~\ref{sec:intro}:
\begin{enumerate}
\item If we have found a policy tree that is known to be optimal we
  can terminate the algorithm.
\item If we have a policy tree $f^{*}(d,N)$ that is known to be
  optimal for some set of units $N$ and we know that $f^{*}(d,N)$ is
  also optimal for some other set of units $N'$ (i.e.\  $f^{*}(d,N) =
  f^{*}(d,N')$) then we do not need to find $f^{*}(d,N')$ by recursive
  computation.
\item If for two splits $s, s' \in S$ we know that $R^{*}_{d-1,s_{L}(N)} + R^{*}_{d-1,s_{R}(N)} 
  < R^{*}_{d-1,s'_{L}(N)} + R^{*}_{d-1,s'_{R}(N)}$ then there is no
  need to compute either  $R^{*}_{d-1,s_{L}(N)}$ or
  $R^{*}_{d-1,s_{R}(N)}$, since we know $s$ cannot be a split in an
  optimal tree for dataset $N$ (since it is `beaten' by $s'$).
\end{enumerate}

\subsection{Finding perfect trees}
\label{sec:perfect}

Regarding the first optimisation, let us call a policy tree for a
dataset of units \emph{perfect} if it assigns each unit its maximal
reward. It is obvious that a perfect policy tree is optimal, so we can
stop the search for an optimal tree if we ever come across one that is
perfect. Note that for any dataset there will be always be a tree
depth that will allow a perfect tree, and also that perfect trees are
more likely to exist for small datasets.

\subsection{Avoiding recursive computation}
\label{sec:avoiding}

In some cases the bounds found in Section~\ref{sec:bounds} can be used
to immediately find optimal trees. For example, let $N_1$ be some
dataset of units and suppose we have found $f^{*}_{d,N_{1}}$ and that
it happens to be the case that this tree assigns the highest reward
action to each unit in a disjoint dataset of units $N_2$, i.e.
$\forall i \in N_{2} : r(f^{*}_{d,N_{1}},i) = \max_{a \in A}
r(i,a)$. Let $N = N_{1} \dot\cup N_{2}$, then, in this case
$R(f^{*}_{d,N_{1}},N) = R^{*}_{d,N_{1}} + \sum_{i \in N_{2}} \max_{a
  \in A} r(i,a)$ and so by (\ref{eq:llb}) $f^{*}_{d,N_{1}}$ is an
optimal tree for $N$, i.e.\ $f^{*}_{d,N} = f^{*}_{d,N_{1}}$.

Similarly, suppose we have found $f^{*}_{d,N}$ and that
it happens to be the case that this tree assigns the lowest reward
action to each unit in $N_2$, then from (\ref{eq:llb}) we have $R^{*}_{d,N_{1}}
\leq R(f^{*}_{d,N},N_{1})$, from which if follows that $f^{*}_{d,N_{1}} = f^{*}_{d,N}$.


\subsection{todo}
\label{sec:todo}



Suppose we have found $f^{*}_{d,N_{1}}$
and then add $N_2$ to $N_{1}$ to get $N$ then (\ref{eq:ub}) provides
an upper bound on $R^{*}_{d,N}$. Similarly, if we have found
$f^{*}_{d,N}$ and remove $N_2$ from $N$ to get $N_1$, (\ref{eq:lb})
provides an upper bound on $R^{*}_{d,N_{1}}$.


\section{Complex junk}
\label{sec:junk}


Let $s$ be some split and let $s'$ be some other split where
\begin{eqnarray}
  \label{eq:splits}
  s'_{L}(N) & = & s_{L}(N) \dot\cup N' \\
  s'_{R}(N) & = & s_{R}(N) \setminus N' 
\end{eqnarray}
so split $s'$ is arrived at by starting from split $s$ and moving the
units $N'$ from the right side to left.
We have:
\begin{eqnarray}
  \label{eq:splitsub}
  R^{*}_{d,s'_{L}(N)} & \leq & R^{*}_{d,s_{L}(N)} +  \sum_{i \in N'}
                               \max_{a \in A} r(i,a) \\
  R^{*}_{d,s'_{R}(N)} & \leq & R^{*}_{d,s_{R}(N)} -  R(f^{*}_{d,s'_{R}(N)},N')
\end{eqnarray}
and
\begin{eqnarray*}
  \label{eq:sumub}
  R^{*}_{d,s'_{L}(N)} +  R^{*}_{d,s'_{R}(N)}
  & \leq
  &  R^{*}_{d,s_{L}(N)}
    +  R^{*}_{d,s_{R}(N)}
    +  \sum_{i \in N'} \max_{a \in A} r(i,a)
    -  \sum_{i \in N'} \min_{a \in A} r(i,a) \\
  & \leq
  & R^{*}_{d,s_{L}(N)}
    +  R^{*}_{d,s_{R}(N)}
    +  \sum_{i \in N'} \left[ \max_{a \in A} r(i,a)- \min_{a \in A}
    r(i,a) \right]
\end{eqnarray*}


% Let $f^*$ be some arbitrary action assignment function then clearly:
% \begin{equation}
%   \label{eq:addfun}
%  \max_{f \in {\cal F}_d \cup \{f^*\}} \sum_{i \in N} r(i,f(i)) \geq \max_{f \in {\cal F}_d} \sum_{i \in N} r(i,f(i)) 
% \end{equation}
% since having an extra function to choose from never hurts.

Let $f \in {\cal F}_d$. Define $f_{(j,a)}$ as follows:
$f_{(j,a)}(j) = a$ and if $i\neq j$ then
$f_{(j,a)}(i) = f(i)$. So the only difference, if any, between $f$ and
$f_{(j,a)}$ is that $f_{(j,a)}$ assigns action $a$ to unit $j$.
Note that $f_{(j,f(j))} = f$.
Define ${\cal F}_d(j) = \{f_{(j,a)}: f \in {\cal
  F}_{d}, a \in A\}$. Since  ${\cal F}_d \subseteq {\cal F}_d(j)$
we have:
 \begin{equation}
   \label{eq:addfun}
  \max_{f \in {\cal F}_d(j)} R(f,N) \geq \max_{f \in {\cal F}_d}
  R(f,N) = R^{*}_{d,N}
\end{equation}
We also have:
\begin{eqnarray}
  \label{eq:addfun2}
  \max_{f \in {\cal F}_d(j)} R(f,N)
  & =
  &\max_{f \in {\cal F}_d(j)} \sum_{i \in N} r(i,f(i)) \\
  & =
  & \max_{f \in {\cal F}_d(j)} \left[   \sum_{i \in N \setminus
    \{j\} } r(i,f(i)) + r(j,f(j)) \right] \\
  & =
  & \max_{f \in {\cal F}_d(j)} \left[   \sum_{i \in N \setminus
    \{j\} } r(i,f(i)) + \max_{a
    \in A} r(j,a) \right] \\
  & = 
  & \max_{f \in
  {\cal F}_d(j)} \sum_{i \in N \setminus \{j\}} r(i,f(i)) +  \max_{a
    \in A} r(j,a) \\
  & = 
  & \max_{f \in
  {\cal F}_d} \sum_{i \in N \setminus \{j\}} r(i,f(i)) +  \max_{a
    \in A} r(j,a) \\
  & =
  & R^{*}_{d,N \setminus \{j\}} +  \max_{a
    \in A} r(j,a) 
\end{eqnarray}
So
\begin{equation}
  \label{eq:maineq}
  R^{*}_{d,N \setminus \{j\}} 
  +  \max_{a
    \in A} r(j,a) \geq   R^{*}_{d,N } 
\end{equation}
% This means that if we have computed $R^{*}_{d,N \setminus \{j\}}$ and
% then add $j$ to $N \setminus \{j\}$ to get $N$ then the best we can
% hope for is $R^{*}_{d,N \setminus \{j\}} + \max_{a \in A} r(j,a)$. If
% $f^{*}_{d,N \setminus \{j\}}$, the best depth $d$ tree for unit-set
% $N \setminus \{j\}$, happens to assign $j$ to an action with maximal
% reward then it follows that
% $f^{*}_{d,N \setminus \{j\}} = f^{*}_{d,N}$ and there is no need to
% spend time finding $f^{*}_{d,N}$, we already have that tree.


Also
\begin{eqnarray}
  \label{eq:otherbound}
  R^{*}_{d,N}
  & \geq
  & R(f^{*}_{d,N \setminus \{j\}},N)\\
  & =
  & r(j,f^{*}_{d,N\setminus \{j\}}(j))  + \sum_{i \in N
    \setminus\{j\}} r(i,f^{*}_{d,N\setminus \{j\}}(i)) \\
  & =
  & r(j,f^{*}_{d,N\setminus \{j\}}(j))  +   R^{*}_{d,N
    \setminus \{j\}}
\end{eqnarray}
So
\begin{equation}
  \label{eq:main}
  R^{*}_{d,N \setminus \{j\}}
  + r(j,f^{*}_{d,N\setminus \{j\}}(j))
  \leq
  R^{*}_{d,N}
  \leq
  R^{*}_{d,N \setminus \{j\}} + \max_{a
    \in A} r(j,a)
\end{equation}
So if
$r(j,f^{*}_{d,N\setminus \{j\}}(j)) = \max_{a \in A}
r(j,a)$, i.e.\ if $f^{*}_{d,N\setminus \{j\}}$, the best tree
for unit-set $N\setminus \{j\}$, happens to choose the best action
for $j$ then $R^{*}_{d,N} = R^{*}_{d,N \setminus \{j\}} +
r(j,f^{*}_{d,N\setminus \{j\}}(j))$ and so
$f^{*}_{d,N} = f^{*}_{d,N\setminus \{j\}}$


\section{Basic algorithm}
\label{sec:basics}



We assume we have $n$ datapoints, each of which has $p$ covariate
values, followed by $d$ rewards for the $d$ possible actions. Our goal
is to construct an policy tree of depth $\depth$ for this data which has
maximal reward. We will use $i$ to index units, $j$ to index
covariates and $k$ to index actions/rewards. Let $x_{i,j}$ the $j$th
covariate value for unit i and $y_{i,k}$ the the $k$th reward for unit $i$.

A tree of depth 0 has no splits and specifies the same action for each
unit. Let $r(\depth,I)$ be the maximal reward for a tree of depth
$\depth$ for a subset of the data $\{(x_{i},y_{i}): i \in I\}$ then we
have:

\begin{equation}
  \label{eq:rewardt0}
  r(0,I) = \max_{k=1,\dots d} \sum_{i \in I} y_{i,k}
\end{equation}
Note that $r(0,I)$ does not depend on $x$, the covariate values.

Trees of non-zero depth have splits which are specified by a choice of
covariate $j$ and splitting value $\splitval \in \splitvals(j)$, where
$\splitvals(j)$ is the set of valid split points for covariate
$j$. Units $i$ where $x_{i,j} \leq \splitval$ go left and those where
$x_{i,j} > \splitval$ go right. Let
$L(j,\splitval,I) = \{i \in I :x_{i,j} \leq \splitval\}$ and let
$R(j,\splitval,I) = \{i \in I :x_{i,j} > \splitval\}$. We have the
fundamental recursive formula:

\begin{equation}
  \label{eq:recurse}
  r(\depth,I) = \max_{j} \max_{\splitval \in \splitvals(j)} \left\{ r(\depth-1,L(j,\splitval,I))
  + r(\depth-1,R(j,\splitval,I))\right\}, \;\; \depth > 0 
\end{equation}

Equation (\ref{eq:recurse}) suggest a simple dynamic programming
algorithm to find optimal trees: simply consider each possible split,
recursively compute the two rewards for the two datasets created by the
split and return whatever sum of rewards is largest. By simply
recording the split $(j,\splitval)$ which was maximising (for each
depth) we can return the optimal tree.

\section{Implementation issues}
\label{sec:implementation}

It is important to be able to construct the index sets
$L(j,\splitval,I)$ and $R(j,\splitval,I)$ quickly from any given index
set $I$. The \texttt{policytree} package takes the sensible option of
maintaining, for any index set $I$, an ordering of $I$:
$(i_{j,1}, \dots, i_{j,|I|})$ for each covariate $j$ such that
$i_{j,\ell} < i_{j,\ell'} \rightarrow x_{i_{j,\ell},j} \leq
x_{i_{j,\ell'},j}$. In words: $I$ has $p$ orderings, one for each
covariate $j$; the ordering for covariate $j$ is such that indices for
datapoints with smaller values for covariate $j$ always come before
indices for datapoints with larger values for covariate $j$. This data
structure is called \texttt{sorted\_sets} in the \texttt{policytree}
implementation \cite{sverdrup2020}.



The advantage of doing this is as follows. For any $j$
we can order split values $\theta$. Suppose we have $L(j,\splitval,I)$
and $R(j,\splitval,I)$ for some split value $\splitval$ and suppose
$\splitval'$ is the `next' split value for covariate $j$ and index set
$I$. Then to get
$L(j,\splitval',I)$ and $R(j,\splitval',I)$ it is enough to `move'
indices $i$ such that $x_{i,j} \leq \splitval'$ from $R(j,\splitval,I)$
to $L(j,\splitval,I)$ and that will produce $L(j,\splitval',I)$ and
$R(j,\splitval',I)$. Moreover, these indices will be at the start of
the ordering of $I$ for $j$. So to find them we just need to scan the
$j$-ordering of $R(j,\splitval,I)$ until we hit an index for which
$x_{i,j} > \splitval'$. Updating the $j$-ordering is
simple. However, if we are going to do a recursive call we need to
update the $j'$-ordering for both left and right set for all other $j'
\neq j$.


This updating of other $j'$-orderings can be avoided if $\depth = 1$
since we can write:
\begin{equation}
  \label{eq:depthone}
  r(1,I) = \max_{j}\max_{\splitval \in \splitvals(j)} \left\{ \max_{k=1,\dots d} \sum_{i \in
     L(j,\splitval,I) } y_{i,k} + \max_{k=1,\dots d} \sum_{i \in R(j,\splitval,I)} y_{i,k} \right\}
\end{equation}
We can assume that $I$ has a $j$-ordering for each covariate $j$.  In
the case of $D=1$, to
compute $r(1,I)$ we can consider each covariate $j$ in turn and there
is no need to update other orderings for other $j'$. For each $j$ we
consider each split point $\splitval \in \splitvals(j)$ in order. When
we move from $\splitval \in \splitvals(j)$ to the next
$\splitval' \in \splitvals(j)$, we just add $y_{i,k}$ to a `left'
running total for $\sum y_{i,k}$ for each action $k$ and each index
$i$ that was `moved' from right to left. Similarly we subtract each of
these $y_{i,k}$ values from the corresponding running totals on the
right. Note that no index actually
has to be moved and that neither $L(j,\splitval,I)$ nor
$R(j,\splitval,I)$ need be represented explicitly: the indices that we
want are initial ones from $R(j,\splitval,I)$, it is enough to have
$I$ represented explicitly (e.g. as an array of integers) and update
a value specifying the last element of $L(j,\splitval,I)$ and the
first element of $R(j,\splitval,I)$.   The \texttt{policytree} package
contains this optimisation implemented in the C++ function called
\verb+level_one_learning+. Note that with this optimisation the only
time (\ref{eq:rewardt0}) (implemented as the C++ function
\verb+level_zero_learning+ in \texttt{policytree}) would be used is if
a depth 0 tree (i.e.\ a single leaf) was required.

\subsection{Ordered set data structures}
\label{sec:orderedsets}

There are a number of data structures that can be used to represent
the ordered sets $(i_{j,1}, \dots, i_{j,|I|})$. There are 3 main
operations that are performed on these ordered sets:
\begin{description}
\item[Insertion] Inserting one or more new elements into an ordered
  set. This is performed on left-sets each time we move to the next
  split point.
\item[Deletion] Removing one or more new elements from an ordered
  set. This is performed on right-sets each time we move to the next
  split point.
\item[Iteration] To compute (\ref{eq:depthone}) it is necessary to
  access each element of the set in order.
\end{description}

\subsubsection{\texttt{policytree} approach}
\label{sec:policytreeos}

\policytree{} implements ordered sets as \texttt{flat\_set} objects
supplied from the boost library. Each ordered set has an associated
comparison object, which is a total order of the elements in the
set. An ordered set for covariate $j$ is ordered using the values for
that covariate (with a mechanism for tie-breaking if two units have
the same value for covariate $j$). A boost \texttt{flat\_set} is
implemented essentially like a C++ vector (or C array): elements of
the set are stored in order in a single contiguous block of
memory. This makes iteration faster than, for example, a binary tree,
but insertion and deletion are slower.

Points are deleted using the \texttt{erase} method with the position
of element to be erased given. The \policytree{} documentation (i.e.\
comment at line 308 in \verb+tree_search.cpp+) states that this method
takes $O(1)$ (i.e.\ constant) time, but the boost
documentation\footnote{https://www.boost.org/doc/libs/1\_64\_0/doc/html/boost/container/flat\_set.html}
states that it takes time ``Linear to the elements with keys bigger
than [that erased]''. In other words $O(n)$. This is because once the
element has been erased, the elements with bigger keys have to be
moved `leftwards' to fill in the gap left by the erased element. Note
also that it is only for the covariate $j$ whose splits are currently
being considered that the element position is readily available. For
each of the other $p-1$ covariates an additional $O(\log n)$ time is
required to find the element's position in the ordered set.

Points are inserted using the \texttt{insert} method. The
\policytree{} documentation states that this takes $O(\log n)$ time to
execute. But this is only the time it takes to find the position to
insert the element. Once this position is found it is necessary to
shunt elements `rightwards' to make room for the new element. As the
boost documentation states the time requred is: ``Logarithmic search
time plus linear insertion to the elements with bigger keys than [the
inserted element].`` In other words $O(n + \log n) = O(n)$

Iteration over \texttt{flat\_set} is fast, since the elements are
simply stored one after the other. This fast iteration is the stated
reason for the use of flat sets in \policytree.


\subsubsection{Binary flag approach}
\label{sec:binaryflag}

An alternative approach is to create $j$-orderings only for the
complete index set for all data (rather than also for subsets of data
that reach various nodes). To indicate that an index $i$ is in some
index set $I$ we just set \verb+flag[i]=1+ where \verb+flag+ is some
array associated with index set $I$. If $i$ is absent then
\verb+flag[i]=0+. In this approach inserting and deleting elements
takes constant time, since we just need to update the flag value
appropriately. However iterating is slower: to iterate (in order) over
an index set $I$ takes time $O(n)$ rather than $O(|I|)$, since we need
to inspect (the index of) each element in the full data set to see
whether it is present in $I$.

\section{Finding breakpoints}
\label{sec:breakpoints}

Define a breakpoint in a $j$-ordering of an index set $I$ to be a position 
$\ell$, $1 \leq \ell < |I|$ such that $x_{i_{j,\ell},j} <
x_{i_{j,\ell+1},j}$. This means that using $x_{i_{j,\ell},j} =\theta$
as a split value provides a valid split with non-empty left and right
index sets $\{i_{j,1}, \dots, i_{j,\ell}\}$ and $\{i_{j,\ell+1},
\dots, i_{j,|I|}\}$, respectively.

A simple way to find breakpoints is to iterate over all
positions $\ell$ from 1 to $|I|-1$ and check whether
$x_{i_{j,\ell},j} < x_{i_{j,\ell+1},j}$. Note that since we have a
$j$-ordering we always have that $x_{i_{j,\ell},j} \leq
x_{i_{j,\ell+1},j}$ so $x_{i_{j,\ell},j} < x_{i_{j,\ell+1},j}
\Leftrightarrow x_{i_{j,\ell},j} \neq x_{i_{j,\ell+1},j}$.
The problem here is that we inspect each possible position and we have
to do a check.


An alternative approach is to record, for each $j$, the positions of
breakpoints in the $j$-ordering of the full data set. We can then
iterate over these pre-recorded breakpoints. In the full dataset each
time we move from a current breakpoint to the next one there is a non-empty
set of indices that `move' from right to left. For a given index set
each such set may be reduced (due to missing indices). If the moving
index set is empty, then we just skip ahead to the next breakpoint.

If a covariate is discrete, then there may be far fewer breakpoints
than members of an index set. For example, in the case of binary data
there is only one! Pre-recording and later using breakpoints is easy
to implement with the binary flag approach mentioned in
Section~\ref{sec:binaryflag} but we still need to check whether each
index is present when constructing the `moving' indices.

\subsection{Memory allocation}
\label{sec:memory}

During the search for an optimal tree, index sets and
best-trees-so-far must be created and stored. In our C code all space
required for any of these is created once at the start, and then this
space is re-used. In policytree it appears that memory to store data
structures (for example, sorted sets) is allocated many times.


\section{Time complexity of the \texttt{policytree} algorithm}
\label{sec:policytreecomplexity}


In the paper \cite{sverdrup2020} the time complexity of the
\texttt{polictree} algorithm is stated as
$O(p^{D}n^{D}(\log n + d) + pn\log n)$. As Sverdrup \emph{et al}
correctly note the $O(pn\log n)$ term accounts for the one off cost of
constructing the \texttt{sorted\_sets} data structure.

When computing the time complexity for the full algorithm Sverdrup
\emph{et al} state that ``For every single split along along every
dimension we remove a sample from a Binary Search Tree and add to
another; this takes O(log N ) time'' \cite{sverdrup2020}. Insertion
and deletion in a binary search tree actually takes $O(h)$ time
\cite{cormen90:_introd_algor} where $h$ is the height of that tree but
it is true that typically $h = O(\log n)$. But this is irrelevant
since \texttt{policytree} does not use a binary search tree for its
\texttt{sorted\_sets} data structure. Instead, as discussed in
Section~\ref{sec:policytreeos} a \texttt{flat\_set} is used where
insertion and deletion is $O(n)$.

\begin{quotation}
  Sverdrup \emph{et al} basically use the \emph{substitution method}
  to derive that the complexity of their algorithm is
  $O(p^{D}n^{D}(\log n + d)$. But I think their reasoning is faulty:
  although I suspect they have ended up with the correct
  end-result. Perhaps it is not worth spending time on this
  issue. Recurrence is $T(n,D,p) \leq p.n.(2(p-1)n\log n +
  2T(n,D-1,p))$. Without level one learning trick this gives something
  like $O(2^{D}n^{2}p^{2} + 2^{D}nd) = O(2^{D}n(np^{2}+d))$. 
\end{quotation}

\section{Benchmarks}
\label{sec:benchmarks}

Benchmarks using the policytree R package 1.2.0 and C code from commit b431d42 with the executables pt
and ot created as follows.

\begin{verbatim}
gcc -DNDEBUG -O2 pt_opttree.c main.c -o pt
gcc -DNDEBUG -O2 opttree.c main.c -o ot
\end{verbatim}

Experiments conducted on this machine on a 2.7GHz Linux laptop using a
single core. Both datasets used have only 2 actions/rewards. The two
arguments after the filename are the number of actions and the desired
depth of tree, respectively.


\begin{figure}
  \centering
\begin{verbatim}
(base) uw20605@IT079795:~/repos/tailoring$ time ./pt IFLS.txt 2 2 
Actions: 0: "scores.DML" 1: "scores.DR" 
node = 0x55d25e991690
covariate = "past_ind_mis"
value = 0
reward = 69.3649
left_child = 0x55d25e9916d0
right_child = 0x55d25e991790

node = 0x55d25e9916d0
covariate = "province.f18"
value = 0
reward = 32.0115
left_child = 0x55d25e991710
right_child = 0x55d25e991750

node = 0x55d25e991710
reward = 0
action_id = 1
left_child = (nil)
right_child = (nil)

node = 0x55d25e991750
reward = 32.0115
action_id = 0
left_child = (nil)
right_child = (nil)

node = 0x55d25e991790
covariate = "region.f3-Jawa"
value = 0
reward = 37.3533
left_child = 0x55d25e9917d0
right_child = 0x55d25e991810

node = 0x55d25e9917d0
reward = 0
action_id = 1
left_child = (nil)
right_child = (nil)

node = 0x55d25e991810
reward = 37.3533
action_id = 0
left_child = (nil)
right_child = (nil)


real	1m13.226s
user	1m13.194s
sys	0m0.028s
\end{verbatim}
  \caption{C policytree-style (pt\_opttree) on binary data IFLS.txt
    with $p=64$ and $n=10,622$}
\end{figure}


\begin{figure}
  \centering
\begin{verbatim}
(base) uw20605@IT079795:~/repos/tailoring$ time ./ot IFLS.txt 2 2 
Actions: 0: "scores.DML" 1: "scores.DR" 
node = 0x55caeea5b480
covariate = "past_ind_mis"
value = 1
reward = 69.3649
action_id = 0
left_child = 0x55caeea5b4c0
right_child = 0x55caeea5b580

node = 0x55caeea5b4c0
covariate = "province.f18"
value = 1
reward = 32.0115
action_id = 0
left_child = 0x55caeea5b500
right_child = 0x55caeea5b540

node = 0x55caeea5b500
reward = 0
action_id = 1
left_child = (nil)
right_child = (nil)

node = 0x55caeea5b540
reward = 32.0115
action_id = 0
left_child = (nil)
right_child = (nil)

node = 0x55caeea5b580
covariate = "region.f3-Jawa"
value = 1
reward = 37.3533
action_id = 0
left_child = 0x55caeea5b5c0
right_child = 0x55caeea5b600

node = 0x55caeea5b5c0
reward = 0
action_id = 1
left_child = (nil)
right_child = (nil)

node = 0x55caeea5b600
reward = 37.3533
action_id = 0
left_child = (nil)
right_child = (nil)


real	0m0.695s
user	0m0.678s
sys	0m0.012s
\end{verbatim}
  \caption{C binary flag, stored breakpoints (opttree) on binary data IFLS.txt
        with $p=64$ and $n=10,622$}
\end{figure}

\begin{figure}
  \centering
\begin{verbatim}
(base) uw20605@IT079795:~/repos/tailoring$ time Rscript pt.R  IFLS.txt 2 2 
Warning in policy_tree(x, gammas, depth, TRUE) :
  The number of covariates exceeds 50. Consider reducing the dimensionality before running policy_tree, by for example using only the Xj's with the highest variable importance (`grf::variable_importance` - the runtime of exact tree search scales with ncol(X)^depth, see the documentation for details).
policy_tree object 
Tree depth:  2 
Actions:  1: scores.DML 2: scores.DR 
Variable splits: 
(1) split_variable: past_ind_mis  split_value: 0 
  (2) split_variable: province.f18  split_value: 0 
    (4) * action: 2 
    (5) * action: 1 
  (3) split_variable: region.f3.Jawa  split_value: 0 
    (6) * action: 2 
    (7) * action: 1 

real	6m38.703s
user	6m38.374s
sys	0m0.159s
\end{verbatim}
  \caption{policytree on binary data IFLS.txt
    with $p=64$ and $n=10,6225$}
\end{figure}

\begin{figure}
  \centering
\begin{verbatim}
(base) uw20605@IT079795:~/repos/tailoring$ time ./pt fakedata.txt 2 2 
Actions: 0: "control" 1: "treated" 
node = 0x55f2ff0df4e0
covariate = "X1"
value = 0.06
reward = 356.805
left_child = 0x55f2ff0df520
right_child = 0x55f2ff0df5e0

node = 0x55f2ff0df520
covariate = "X2"
value = 1.02
reward = 380.689
action_id = 1
left_child = 0x55f2ff0df560
right_child = 0x55f2ff0df5a0

node = 0x55f2ff0df560
reward = 363.023
action_id = 1
left_child = (nil)
right_child = (nil)

node = 0x55f2ff0df5a0
reward = 17.6663
action_id = 0
left_child = (nil)
right_child = (nil)

node = 0x55f2ff0df5e0
covariate = "X2"
value = -1.26
reward = -23.8845
left_child = 0x55f2ff0df620
right_child = 0x55f2ff0df660

node = 0x55f2ff0df620
reward = 44.219
action_id = 1
left_child = (nil)
right_child = (nil)

node = 0x55f2ff0df660
reward = -68.1035
action_id = 0
left_child = (nil)
right_child = (nil)


real	0m7.897s
user	0m7.790s
sys	0m0.020s
\end{verbatim}
  \caption{C policytree-style (pt\_opttree) on continuous data fakedata.txt
        with $p=10$ and $n=10,000$}
\end{figure}

\begin{figure}
  \centering
\begin{verbatim}
(base) uw20605@IT079795:~/repos/tailoring$ time ./ot fakedata.txt 2 2 
Actions: 0: "control" 1: "treated" 
node = 0x55dbc294a480
covariate = "X1"
value = 0.07
reward = 356.805
action_id = 0
left_child = 0x55dbc294a4c0
right_child = 0x55dbc294a580

node = 0x55dbc294a4c0
covariate = "X2"
value = 1.03
reward = 380.689
action_id = 1
left_child = 0x55dbc294a500
right_child = 0x55dbc294a540

node = 0x55dbc294a500
reward = 363.023
action_id = 1
left_child = (nil)
right_child = (nil)

node = 0x55dbc294a540
reward = 17.6663
action_id = 0
left_child = (nil)
right_child = (nil)

node = 0x55dbc294a580
covariate = "X2"
value = -1.25
reward = -23.8845
action_id = 0
left_child = 0x55dbc294a5c0
right_child = 0x55dbc294a600

node = 0x55dbc294a5c0
reward = 44.219
action_id = 1
left_child = (nil)
right_child = (nil)

node = 0x55dbc294a600
reward = -68.1035
action_id = 0
left_child = (nil)
right_child = (nil)


real	0m10.105s
user	0m10.014s
sys	0m0.025s
\end{verbatim}
  \caption{C binary flag, stored breakpoints (opttree) on continuous
    data fakedata.txt
    with $p=10$ and $n=10,000$}
\end{figure}

\begin{figure}
  \centering
\begin{verbatim}
(base) uw20605@IT079795:~/repos/tailoring$ time Rscript pt.R  fakedata.txt 2 2 
policy_tree object 
Tree depth:  2 
Actions:  1: control 2: treated 
Variable splits: 
(1) split_variable: X1  split_value: 0.06 
  (2) split_variable: X2  split_value: 1.02 
    (4) * action: 2 
    (5) * action: 1 
  (3) split_variable: X2  split_value: -1.26 
    (6) * action: 2 
    (7) * action: 1 

real	0m15.431s
user	0m15.129s
sys	0m0.160s
\end{verbatim}
  \caption{policytree on continuous data fakedata.txt
    with $p=10$ and $n=10,000$}
\end{figure}

Here are perf analyses on IFLS.txt. For both policytree and the pt C
version examining the top-listed item (0x00000.. 733) indicates that
most time is spent copying memory via memcpy. The 10\% on creating
sorted sets is probably due to time spent allocating memory.

\begin{figure}
  \centering
\begin{verbatim}
sudo perf record -e cpu-clock ./pt IFLS.txt 2 2
 62.36%  pt       libc-2.31.so       [.] 000000000018b733
 25.30%  pt       pt                 [.] find_best_split       

sudo perf record -e cpu-clock ./ot IFLS.txt 2 2
  50.27%  ot       ot                 [.] next_breakpoint
  15.88%  ot       ot                 [.] find_best_split
   5.92%  ot       ot                 [.] tree_search
   4.55%  ot       libc-2.31.so       [.] __vfwscanf_internal
   2.89%  ot       libc-2.31.so       [.] _IO_wfile_overflow
   2.89%  ot       ot                 [.] getnfields

sudo perf record -e cpu-clock Rscript pt.R IFLS.txt 2 2 
  74.77%  R        libc-2.31.so         [.] 0x000000000018b733
  10.02%  R        policytree.so        [.] .. create_sorted_sets
   2.59%  R        libc-2.31.so         [.] 0x000000000018b94b
   2.54%  R        libc-2.31.so         [.] 0x000000000018b941
   2.47%  R        libc-2.31.so         [.] 0x000000000018b93c
   2.41%  R        libc-2.31.so         [.] 0x000000000018b946
   1.74%  R        policytree.so        [.] find_best_split
\end{verbatim}
  \caption{perf analyses on binary data}
  \label{fig:perfbinary}
\end{figure}


\begin{figure}
  \centering
\begin{verbatim}
sudo perf record -e cpu-clock ./pt fakedata.txt 2 2
  85.90%  pt       pt                 [.] find_best_split
   7.19%  pt       libc-2.31.so       [.] 0x000000000018b733

sudo perf record -e cpu-clock ./ot fakedata.txt 2 2
  56.94%  ot       ot                 [.] next_breakpoint
  41.97%  ot       ot                 [.] find_best_split

sudo perf record -e cpu-clock Rscript pt.R fakedata.txt 2 2
  55.91%  R        policytree.so        [.] level_one_learning
  17.10%  R        libc-2.31.so         [.] 0x000000000018b733
   6.27%  R        policytree.so        [.] ... create_sorted_sets(Data const*, bool)
   2.89%  R        libc-2.31.so         [.] 0x000000000018b94b
   2.78%  R        libc-2.31.so         [.] 0x000000000018b941
   2.77%  R        libc-2.31.so         [.] 0x000000000018b946
   2.57%  R        libc-2.31.so         [.] 0x000000000018b93c
   1.56%  R        policytree.so        [.] find_best_split
\end{verbatim}
  \caption{perf analyses on continuous data}
  \label{fig:perfcontinuous}
\end{figure}

\bibliographystyle{plain}
\bibliography{coding}


\end{document}
